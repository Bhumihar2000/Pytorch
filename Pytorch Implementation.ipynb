{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a81eb06",
   "metadata": {},
   "source": [
    "### Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1368d7c4",
   "metadata": {},
   "source": [
    "Tensor is similar to Numpy's ndarray, the additional point for tensors in we can use it in GPUs to accelerate computing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "201a0794",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cda893c",
   "metadata": {},
   "source": [
    "Note:-  Uninitialized matrix is declared, but does't contain definite known values before it is used. When we created un Unintialized matrix whatever values were allocated inside the memory will apear as the initial values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328f4300",
   "metadata": {},
   "source": [
    "Construct 6x3 matrix, uninitialized:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fe450d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.empty(6,3)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3660e8e0",
   "metadata": {},
   "source": [
    "Construct a randomly initialized matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf39ed20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8034, 0.0934, 0.6888],\n",
      "        [0.3241, 0.5029, 0.5854],\n",
      "        [0.7264, 0.2559, 0.0858],\n",
      "        [0.5570, 0.8004, 0.2291]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(4,3)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8118c66",
   "metadata": {},
   "source": [
    "Construct a matrix filled zeros and of dtype long:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09041d51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]])\n",
      "<class 'torch.Tensor'>\n",
      "torch.int64\n"
     ]
    }
   ],
   "source": [
    "a = torch.zeros(4,3, dtype = torch.long)\n",
    "print(a)\n",
    "print(type(a))\n",
    "print(a.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61255cf",
   "metadata": {},
   "source": [
    "### Construct a tensor with data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7dc6312b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7.8000, 5.0000])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([7.8,5])\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c685b29",
   "metadata": {},
   "source": [
    "Or we can create new tensor with existing tensor. These methods we reuse its prpperties of input tensor, e.g. dtype, unless new values are provided by us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e149ceb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7.8000, 5.0000])\n",
      "tensor([[1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(a)\n",
    "a = a.new_ones(6,5, dtype = torch.double)# new methos take in size.\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1bd2dca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0063,  0.1055, -0.7122, -1.1404, -0.0680],\n",
      "        [-0.7920, -0.4047,  1.8657, -0.2069,  1.2807],\n",
      "        [ 1.1231, -1.2583, -0.2503,  0.9851,  0.3088],\n",
      "        [-1.8640,  0.6418,  0.7276,  1.0641, -0.4141],\n",
      "        [ 0.6147,  0.1966, -0.3201,  0.1480, -0.7002],\n",
      "        [ 0.9660, -0.8856, -1.2558,  0.0765,  0.9836]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn_like(a, dtype = torch.float) # Override dtype\n",
    "print(a) # result will be the same size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a999559d",
   "metadata": {},
   "source": [
    "Let's get the size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "85c2fcb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 5])\n"
     ]
    }
   ],
   "source": [
    "print(a.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619d30be",
   "metadata": {},
   "source": [
    "Note:- torch_size is actually a tuple, so it supports all tuple operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e36d257",
   "metadata": {},
   "source": [
    "### Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b8693e",
   "metadata": {},
   "source": [
    "There are multiple syntaxes for operations. In the following examples, we used addition operation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877986fd",
   "metadata": {},
   "source": [
    "Addition: Syntax 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "57e8f710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0063,  0.1055, -0.7122, -1.1404, -0.0680],\n",
      "        [-0.7920, -0.4047,  1.8657, -0.2069,  1.2807],\n",
      "        [ 1.1231, -1.2583, -0.2503,  0.9851,  0.3088],\n",
      "        [-1.8640,  0.6418,  0.7276,  1.0641, -0.4141],\n",
      "        [ 0.6147,  0.1966, -0.3201,  0.1480, -0.7002],\n",
      "        [ 0.9660, -0.8856, -1.2558,  0.0765,  0.9836]])\n"
     ]
    }
   ],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "757fa3e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.6181,  0.9772, -0.1690, -0.3744,  0.0573],\n",
      "        [-0.7637, -0.2883,  2.0585,  0.6161,  1.3623],\n",
      "        [ 1.9069, -1.1586,  0.0023,  1.3490,  0.5960],\n",
      "        [-1.6303,  1.0654,  1.4319,  1.8383,  0.3667],\n",
      "        [ 0.9387,  0.5822,  0.6499,  0.7336, -0.2308],\n",
      "        [ 1.6057, -0.6942, -1.0637,  0.8349,  1.0430]])\n"
     ]
    }
   ],
   "source": [
    "b = torch.rand(6,5)\n",
    "print(a+b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c13e0fd",
   "metadata": {},
   "source": [
    "Addition: Syntax 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a8bd922d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0063,  0.1055, -0.7122, -1.1404, -0.0680],\n",
      "        [-0.7920, -0.4047,  1.8657, -0.2069,  1.2807],\n",
      "        [ 1.1231, -1.2583, -0.2503,  0.9851,  0.3088],\n",
      "        [-1.8640,  0.6418,  0.7276,  1.0641, -0.4141],\n",
      "        [ 0.6147,  0.1966, -0.3201,  0.1480, -0.7002],\n",
      "        [ 0.9660, -0.8856, -1.2558,  0.0765,  0.9836]])\n",
      "tensor([[0.6244, 0.8717, 0.5432, 0.7661, 0.1253],\n",
      "        [0.0283, 0.1165, 0.1928, 0.8230, 0.0816],\n",
      "        [0.7838, 0.0997, 0.2527, 0.3639, 0.2872],\n",
      "        [0.2337, 0.4236, 0.7043, 0.7742, 0.7808],\n",
      "        [0.3240, 0.3857, 0.9700, 0.5857, 0.4694],\n",
      "        [0.6397, 0.1914, 0.1921, 0.7584, 0.0594]])\n"
     ]
    }
   ],
   "source": [
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "004ed89d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.6181,  0.9772, -0.1690, -0.3744,  0.0573],\n",
      "        [-0.7637, -0.2883,  2.0585,  0.6161,  1.3623],\n",
      "        [ 1.9069, -1.1586,  0.0023,  1.3490,  0.5960],\n",
      "        [-1.6303,  1.0654,  1.4319,  1.8383,  0.3667],\n",
      "        [ 0.9387,  0.5822,  0.6499,  0.7336, -0.2308],\n",
      "        [ 1.6057, -0.6942, -1.0637,  0.8349,  1.0430]])\n"
     ]
    }
   ],
   "source": [
    "print(torch.add(a,b))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762a990d",
   "metadata": {},
   "source": [
    "Addition: Provisiong an output as an arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6eaa340a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.6181,  0.9772, -0.1690, -0.3744,  0.0573],\n",
      "        [-0.7637, -0.2883,  2.0585,  0.6161,  1.3623],\n",
      "        [ 1.9069, -1.1586,  0.0023,  1.3490,  0.5960],\n",
      "        [-1.6303,  1.0654,  1.4319,  1.8383,  0.3667],\n",
      "        [ 0.9387,  0.5822,  0.6499,  0.7336, -0.2308],\n",
      "        [ 1.6057, -0.6942, -1.0637,  0.8349,  1.0430]])\n"
     ]
    }
   ],
   "source": [
    "result = torch.empty(6,5)\n",
    "torch.add(a,b, out = result)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81178c03",
   "metadata": {},
   "source": [
    "Addition:- in place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c39c97b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.6181,  0.9772, -0.1690, -0.3744,  0.0573],\n",
      "        [-0.7637, -0.2883,  2.0585,  0.6161,  1.3623],\n",
      "        [ 1.9069, -1.1586,  0.0023,  1.3490,  0.5960],\n",
      "        [-1.6303,  1.0654,  1.4319,  1.8383,  0.3667],\n",
      "        [ 0.9387,  0.5822,  0.6499,  0.7336, -0.2308],\n",
      "        [ 1.6057, -0.6942, -1.0637,  0.8349,  1.0430]])\n"
     ]
    }
   ],
   "source": [
    "# adds a to b\n",
    "b.add_(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba7328d",
   "metadata": {},
   "source": [
    "Note:- Any operation that mutates a tensor in-place is post-fixed with an. for example:- a.copy(b), a.b_() will change a."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f7e5f2",
   "metadata": {},
   "source": [
    "We can use standard Numpy-like indexing with all bells and whistles!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ca8be84e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.7122,  1.8657, -0.2503,  0.7276, -0.3201, -1.2558])\n"
     ]
    }
   ],
   "source": [
    "print(a[:,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c36791",
   "metadata": {},
   "source": [
    "Resizing:- We can resize or reshape tensor, use tensor.view for that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4a8f470b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 3])\n",
      "torch.Size([9])\n",
      "torch.Size([1, 9])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(3,3)\n",
    "b = a.view(9)\n",
    "c = a.view(-1,9) # The size -1 is inferred  from other dimesions\n",
    "print(a.size())\n",
    "print(b.size())\n",
    "print(c.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65644ed3",
   "metadata": {},
   "source": [
    "If you have one value tensor, use.item() to get the value of the python number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "93a584e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.6473])\n",
      "-0.6472882628440857\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(1)\n",
    "print(a)\n",
    "print(a.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af06159b",
   "metadata": {},
   "source": [
    "### Numpy Bridge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f269fe",
   "metadata": {},
   "source": [
    "Converting a torch tensor to numpy array and vice versa is breeze"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eeb9fbf",
   "metadata": {},
   "source": [
    "The Torch Tensor and Numpy array will share thweir underlying memory locations (if the torch tensor is on CPU), and changing one will change the other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6203a0e",
   "metadata": {},
   "source": [
    "### Converting a Torch Tensor to Nupy Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9ff1de43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(4)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d87ac801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "y = x.numpy()\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cd405a",
   "metadata": {},
   "source": [
    "See How numpy array changed in value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9be8b39f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 2., 2., 2.])\n",
      "[2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "x.add_(1)\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf182d1",
   "metadata": {},
   "source": [
    "### Converting Numpy Array to Torch Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67879bc",
   "metadata": {},
   "source": [
    "lets see how changing the numpy array chanmged the torch tensor automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "22b59285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 2. 2. 2.]\n",
      "tensor([2., 2., 2., 2.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "f = np.ones(4)\n",
    "g = torch.from_numpy(f)\n",
    "np.add(f,1,out = f)\n",
    "print(f)\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2319442",
   "metadata": {},
   "source": [
    "All the tensor on the cpu except a charTensor Support converting to Numpy and back."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29a56a1",
   "metadata": {},
   "source": [
    "## Tensors Can be Moved Onto Any Device Using The .to method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a39a951",
   "metadata": {},
   "source": [
    "## Any device in general CPU and GPU and some special casxe TPU is also there."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11962f1d",
   "metadata": {},
   "source": [
    "## AUTOGRAD:- Automatic Differentiation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "36af99f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a6859c",
   "metadata": {},
   "source": [
    "Create a Tensor and Set requires_grad = True to track cimputation with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d6d6c2e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(2,2, requires_grad = True)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9825f7",
   "metadata": {},
   "source": [
    "Do A Tensor Operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "09242a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 3.],\n",
      "        [3., 3.]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "b = a+2\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699df0be",
   "metadata": {},
   "source": [
    "b was created as a result of an operation, so it has a grad_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0c23a279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<AddBackward0 object at 0x0000012AF83A6400>\n"
     ]
    }
   ],
   "source": [
    "print(b.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9901d3",
   "metadata": {},
   "source": [
    "Do More Operation On b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "260793f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[27., 27.],\n",
      "        [27., 27.]], grad_fn=<MulBackward0>) tensor(27., grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "c = b * b* 3\n",
    "out = c.mean()\n",
    "\n",
    "print(c, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c29165",
   "metadata": {},
   "source": [
    ".requires_grad_() changes an existing Tensor's requires_grad flag in-place.\n",
    "The Input Flag Defaults to false if not given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f3fcee4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "p = torch.randn(3,3)\n",
    "p = ((p * 3) / (p-1))\n",
    "print(p.requires_grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d414eed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "p.requires_grad_(True)\n",
    "print(p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "943c1117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3154.6455, grad_fn=<SumBackward0>)\n",
      "<SumBackward0 object at 0x0000012AF37CA710>\n"
     ]
    }
   ],
   "source": [
    "q = (p * p).sum()\n",
    "print(q)\n",
    "print(q.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea24e3b",
   "metadata": {},
   "source": [
    "### Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ab04d4",
   "metadata": {},
   "source": [
    "Let's backdrop now. Because out contains a single scaler, out.backword is equivalent to out.backward(torch.tensor(1.))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d9f21364",
   "metadata": {},
   "outputs": [],
   "source": [
    "out.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2cc568",
   "metadata": {},
   "source": [
    "print gradients d(out)/dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7d64c55a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n",
      "tensor([[4.5000, 4.5000],\n",
      "        [4.5000, 4.5000]])\n"
     ]
    }
   ],
   "source": [
    "print(a)\n",
    "print(a.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb636f8",
   "metadata": {},
   "source": [
    "you should have got a matrix of 4.5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb61ced0",
   "metadata": {},
   "source": [
    "### Mathematically - Jacobians and Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7c7895",
   "metadata": {},
   "source": [
    "Now Let's Have a look at an example of vector - jacobian product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ea72e8da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  408.5759, -1318.5188, -1170.6854], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad = True)\n",
    "\n",
    "y = x * 2\n",
    "\n",
    "while y.data.norm() < 1000:\n",
    "    y = y * 2\n",
    "    \n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb34b4b8",
   "metadata": {},
   "source": [
    "Now in this case y is no longer a scalar. torch.autograd(torch.tensor(1.)) could not compute the full jacobian directly, but if we just want the vector- jacobian product simply pass the vector to backward as argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ffcad545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0240e+02, 1.0240e+03, 1.0240e-01])\n"
     ]
    }
   ],
   "source": [
    "v = torch.tensor([0.1, 1.0, 0.0001], dtype = torch.float)\n",
    "y.backward(v)\n",
    "\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c155e6",
   "metadata": {},
   "source": [
    "You can also stop autograd from tracking history on Tesors with .requires_grad = True either by wrapping the code block in with torch.no_grad():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1f10073f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(x.requires_grad)\n",
    "print((x ** 2).requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    print(( x ** 2).requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e862b784",
   "metadata": {},
   "source": [
    "or by using .detach() to get a new Tensor with the same content but that does not require gradients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "23a4079a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "tensor(True)\n"
     ]
    }
   ],
   "source": [
    "print(x.requires_grad)\n",
    "y = x.detach()\n",
    "print(y.requires_grad)\n",
    "print(x.eq(y).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88bb5246",
   "metadata": {},
   "source": [
    "### Define The Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325d0644",
   "metadata": {},
   "source": [
    "Let's Define The Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fabbe630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "network()\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class network(nn.Module):\n",
    "    \n",
    "    def __int__(self):\n",
    "        super(network, self).__init__()\n",
    "        # 1 Input image channel, 6 output channels, 3x3 square convolution\n",
    "        # Kernel\n",
    "        self.conv1 = nn.Conv2d(1,6,3)\n",
    "        self.conv2 = nn.Conv2d(6,16,3)\n",
    "        # an affine operation: y = mx + b\n",
    "        self.fc1 = nn.Linear(16 * 6 * 6, 120)# 6*6 from image dimesions\n",
    "        self.fc2 = nn.Linear(120,84)\n",
    "        self.fc3 = nn.Linear(84,10)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        # Max Pooling over a (2,2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2,2))\n",
    "        # If the size is a square you can only specify a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def num_flat_features(self,x):\n",
    "        size = x.size()[1:] # All Dimesions except the batch dimesion\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "    \n",
    "    \n",
    "net = network()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fdc91f2",
   "metadata": {},
   "source": [
    "You just have to define the forward and the backward function (where gradients are computed) is automatically defined for you using autograd. You can use ant of the tensor operations in the forward function.\n",
    "\n",
    "The Learnable parameters of a model are returened by net.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "78704dce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "params = list(net.parameters())\n",
    "print(len(params))\n",
    "print(params) # Conv1's.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90188c4b",
   "metadata": {},
   "source": [
    "Let's try a random 32x32 input. Note:- Expected input size of this net(LeNet) is 32x32. To use this net on the MNIST dataset, please resize the images from the dataset to 32x32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1b8fd582",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'network' object has no attribute 'conv1'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-56-490bba485a50>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0minp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pythorch_python\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-51-aca3543e0bd3>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;31m# Max Pooling over a (2,2) window\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m         \u001b[1;31m# If the size is a square you can only specify a single number\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pythorch_python\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1176\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1177\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[1;32m-> 1178\u001b[1;33m             type(self).__name__, name))\n\u001b[0m\u001b[0;32m   1179\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1180\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Module'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'network' object has no attribute 'conv1'"
     ]
    }
   ],
   "source": [
    "inp = torch.randn(1,1,32,32)\n",
    "out = net(inp)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f229d902",
   "metadata": {},
   "source": [
    "Zero the gradient buffers of all parametrs and backprops with random gradients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2a8c423c",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Mismatch in shape: grad_output[0] has a shape of torch.Size([1, 10]) and output[0] has a shape of torch.Size([]).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-57-6e534881462b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\pythorch_python\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 307\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pythorch_python\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[0mgrad_tensors_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_tensor_or_tensors_to_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m     \u001b[0mgrad_tensors_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_make_grads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mretain_graph\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pythorch_python\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36m_make_grads\u001b[1;34m(outputs, grads)\u001b[0m\n\u001b[0;32m     36\u001b[0m                                    \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" and output[\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m                                    \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"] has a shape of \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m                                    + str(out.shape) + \".\")\n\u001b[0m\u001b[0;32m     39\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_complex\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m                 raise RuntimeError(\"For complex Tensors, both grad_output and output\"\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Mismatch in shape: grad_output[0] has a shape of torch.Size([1, 10]) and output[0] has a shape of torch.Size([])."
     ]
    }
   ],
   "source": [
    "net.zero_grad()\n",
    "out.backward(torch.randn(1,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ec95d4",
   "metadata": {},
   "source": [
    "Note:- torch.nn only supports mini-batches. The entire torch.nn package only supports inputs that are a mini batch of samples and not a single sample. for example nn.Conv2d will take in a 4D tensor of nSamples xnChannels x Height x Width. if you have a single sample  just use input.unsqueeze(0) to add a fake batch dimesion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31725057",
   "metadata": {},
   "source": [
    "### At this point, we covered."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abee10fb",
   "metadata": {},
   "source": [
    " Defined Neural Network, \n",
    " Processing Input and calling Backward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495e4073",
   "metadata": {},
   "source": [
    "### Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3ee867",
   "metadata": {},
   "source": [
    "For Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1a13590e",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'network' object has no attribute 'conv1'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-58-8a21aa6c42f5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# a dummy target for example\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# make it the same shape as output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pythorch_python\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-51-aca3543e0bd3>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;31m# Max Pooling over a (2,2) window\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m         \u001b[1;31m# If the size is a square you can only specify a single number\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pythorch_python\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1176\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1177\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[1;32m-> 1178\u001b[1;33m             type(self).__name__, name))\n\u001b[0m\u001b[0;32m   1179\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1180\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Module'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'network' object has no attribute 'conv1'"
     ]
    }
   ],
   "source": [
    "output = net(inp)\n",
    "target = torch.randn(10) # a dummy target for example\n",
    "target = target.view(1,-1) # make it the same shape as output\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "loss = criterion(output, target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67963ae9",
   "metadata": {},
   "source": [
    "For Illustration, let us follow a few steps backward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "98200c94",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-59-6afbee9a5c98>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# MSELoss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Linear\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext_functions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Relu\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'loss' is not defined"
     ]
    }
   ],
   "source": [
    "print(loss.grad_fn) # MSELoss\n",
    "print(loss.grad_fn.next_functions[0][0]) # Linear\n",
    "print(loss.grad_fn.next_functions[0][0].next_functions[0][0]) # Relu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62498ef4",
   "metadata": {},
   "source": [
    "### Backprop "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d048cac3",
   "metadata": {},
   "source": [
    "Now we shall call loss.backward(), and have a look at conv1's bias gradients before and after the backward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "45b079bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.bias.grad before backward\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'network' object has no attribute 'conv1'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-60-5870a9d0778e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'conv1.bias.grad before backward'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pythorch_python\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1176\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1177\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[1;32m-> 1178\u001b[1;33m             type(self).__name__, name))\n\u001b[0m\u001b[0;32m   1179\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1180\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Module'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'network' object has no attribute 'conv1'"
     ]
    }
   ],
   "source": [
    "net.zero_grad() # zeros the gradient buffers of all parameters\n",
    "\n",
    "print('conv1.bias.grad before backward')\n",
    "print(net.conv1.bias.grad)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print('conv1.bias.grad after backward')\n",
    "print(net.conv1.bia.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87eeb86f",
   "metadata": {},
   "source": [
    "### Update The Weights "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d0b87e",
   "metadata": {},
   "source": [
    "We can implement this using simple python code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b8b48b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "for f in net.parameters():\n",
    "    f.data.sub_(f.grad.data * learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e107a3e",
   "metadata": {},
   "source": [
    "However, as you use neural networks, you want to use various differents update rules such as SGD, Nesterov-SGD, Adam, RMSProp, etc. To enable this we built a small package: torch.optim that implements all these methods. using it is very simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4bd0fe1d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "optimizer got an empty parameter list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-63-8298a2252565>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# create your optimizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.01\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# in your training loop:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pythorch_python\\lib\\site-packages\\torch\\optim\\sgd.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, params, lr, momentum, dampening, weight_decay, nesterov)\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnesterov\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmomentum\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mdampening\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Nesterov momentum requires a momentum and zero dampening\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 95\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSGD\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     96\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setstate__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pythorch_python\\lib\\site-packages\\torch\\optim\\optimizer.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, params, defaults)\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[0mparam_groups\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"optimizer got an empty parameter list\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0mparam_groups\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'params'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mparam_groups\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: optimizer got an empty parameter list"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# create your optimizer\n",
    "optimizer = optim.SGD(net.parameters(), lr = 0.01)\n",
    "\n",
    "# in your training loop:\n",
    "optimizer.zero_grad() # Zero The Gradient Buffers\n",
    "\n",
    "output = net(inp)\n",
    "loss = criterion(output, target)\n",
    "loss.backward()\n",
    "optimizer.step()# Does The Update"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2869086c",
   "metadata": {},
   "source": [
    "Observed How Gradient buffers had to be manually set to zero using optimizer.zero_grad(). This is because gradients are accumulated as explained in the backprop section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4948484a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
